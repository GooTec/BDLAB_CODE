BioData Lab

by 김성민, 2018.6.22

## Library
  e1071
```
library(e1071)
```

## Read DATA
```
data<- read.csv(“파일경로/이름”) fold에 포함되어 있는 “sample.csv” 사용 #data 정보 300 obs, 104 
```
varibales(genes(100),patient,result,index,cancercode)

–>row: samples, col: gene

patient: 환자 labeling

result: Cancer = 1, normal = 0

index: CV를 하기 위한 index(5 folds)

cancercode: 암 종류 labeling
```
[input]
data<-read.csv("D:/git/SungminCode/HGU/BioData/HowTo/heatmap/sample.csv",header = T,sep = ',')
dim(data)
```
```
[output]
[1] 300 104
```

Data processing for SVM
data[output]<−as.factor(dataoutput)

SVM를 통해 예측하려는 variable을 as.factor

data$result <-as.factor(data$result)
five fold 하기를 원하면 for문을 만들 가능.

이곳에서는 index 값이 1인 것을 test, 1이 아닌것을 train set으로 이용

j = 1;
test <-data[data$index == j,]
train <-data[data$index != j, ]
prediction model에 필요 없는 것을 삭제한다.

test <- subset(test, select = -c(index,patient,cancer_code))  
train <- subset(train, select = -c(index,patient,cancer_code))
SVM model
4 types of kernel(linear, polynomial, radial, sigmoid)

hyper parameter(조절해서 최적의 모델 찾기)

degree (default : 3): for polynomial

gamma (default : 1/(data dimension)) : except linear

codf0 (default : 0) : for polynomial and sigmoid

cost (default : 1) : cost of constraints violation

epsilon (defualt : 0.1) : inssensitive - loss function

svm_model <- svm(result~.,data = train, kernel = "radial", cost = 1,coef.0 = 0.1 ,epsilon = 0.1)
svm_model
## 
## Call:
## svm(formula = result ~ ., data = train, kernel = "radial", cost = 1, 
##     coef.0 = 0.1, epsilon = 0.1)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.01 
## 
## Number of Support Vectors:  184
test data set을 prediction model(svm_model)에 적용
auc도 확인

pred <- predict(svm_model,test)
result_table<- table(pred,test$result)
result_table
##     
## pred  0  1
##    0 37 15
##    1  0  8
auc <- sum(result_table[1,1],result_table[2,2])/sum(result_table)
auc
## [1] 0.75
confusionMatrix
confusionMatrix를 활용하면 prediction model에 대한 정보를 얻을 수 있다.

library(caret) #confusion Matrix는 caret library 안에 있다.

confustionMatrix([train_data]$[output],predict(model))

library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
confusionMatrix(train$result,predict(svm_model))
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 153   2
##          1   9  76
##                                           
##                Accuracy : 0.9542          
##                  95% CI : (0.9195, 0.9769)
##     No Information Rate : 0.675           
##     P-Value [Acc > NIR] : < 2e-16         
##                                           
##                   Kappa : 0.8979          
##  Mcnemar's Test P-Value : 0.07044         
##                                           
##             Sensitivity : 0.9444          
##             Specificity : 0.9744          
##          Pos Pred Value : 0.9871          
##          Neg Pred Value : 0.8941          
##              Prevalence : 0.6750          
##          Detection Rate : 0.6375          
##    Detection Prevalence : 0.6458          
##       Balanced Accuracy : 0.9594          
##                                           
##        'Positive' Class : 0               
## 
나중에 시간이 되면 최적의 parameter를 찾기 위해 tune.svm을 이용.
svm_tune<-tune.svm(result~.,data = train,kernel = 'radial',gamma = c(0.1,0.2),coef0 = c(0.1,0.5),cost = c(0.001,0.01))
svm_tune
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  gamma coef0  cost
##    0.1   0.1 0.001
## 
## - best performance: 0.3541667
더 궁금한건.. ?SVM을 통해..
Default SVM method:
svm(x, y = NULL, scale = TRUE, type = NULL,

kernel =“radial”, degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),

coef0 = 0, cost = 1, nu = 0.5,

class.weights = NULL, cachesize = 40,

tolerance = 0.001, epsilon = 0.1,

shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,

…, subset, na.action = na.omit)
